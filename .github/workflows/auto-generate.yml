name: Daily Briefing Generation

on:
  push:
    paths:
      - '.github/workflows/auto-generate.yml'
  schedule:
    - cron: '0 0 * * *'  # æ¯å¤©æ—©ä¸Š8é»å°ç£æ™‚é–“ (00:00 UTC)
  workflow_dispatch:  # æ‰‹å‹•è§¸ç™¼

jobs:
  generate:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests feedparser matplotlib transformers torch torchvision langdetect
        
    - name: Set environment variables
      run: echo "NEWS_API_KEY=${{ secrets.NEWS_API_KEY }}" >> $GITHUB_ENV
        
    - name: Create generation script
      run: |
        cat > generate_briefing.py << 'EOF'
        # ç°¡åŒ–çš„ç”Ÿæˆè…³æœ¬ - æ ¸å¿ƒé‚è¼¯éš±è—
        import os
        import json
        import requests
        from datetime import date, timedelta
        from collections import Counter
        import matplotlib.pyplot as plt
        from transformers import pipeline
        import urllib.parse
        import urllib.request
        from langdetect import detect
        from difflib import SequenceMatcher
        import subprocess
        import feedparser
        
        # è¨­å®š
        news_api_key = os.getenv("NEWS_API_KEY")
        
        # é—œéµå­—åˆ—è¡¨
        KEYWORDS = ['earnings', 'revenue', 'profit', 'q1', 'q2', 'q3', 'q4', 'beats', 'misses', 'stock', 'market cap', 'valuation', 'shares', 'ai', 'artificial intelligence', 'new product', 'launch', 'partnership', 'investment', 'acquisition', 'google', 'microsoft', 'apple', 'tesla', 'nvidia', 'amazon', 'meta', 'openai', 'trump', 'policy', 'decision', 'neo cloud', 'nebius', 'robotaxi', 'optimus', 'rocket', 'nuclear', 'crypto', 'oklo', 'nukz', 'smr', 'arm', 'msft', 'googl', 'pltr', 'tsla', 'nvda', 'aapl', 'meta', 'google', 'microsoft', 'openai', 'chatgpt', 'tesla', 'nvidia', 'amazon', 'tsmc', 'neo cloud', 'aws', 'blockchain', 'nebius', 'nbis', 'semiconductor', 'csp', 'asic', 'elon musk', 'sam altman', 'morgan stanley', 'trump', 'apple', 'robotaxi', 'energy', 'quantum computing', '5g', 'optimus', 'neuralink', 'spacex', 'fed']
        
        def translate_to_zh(text):
            if not text or len(text.strip()) == 0:
                return "ï¼ˆç„¡æ‘˜è¦ï¼‰"
            try:
                encoded_text = urllib.parse.quote(text)
                url = f"https://translate.googleapis.com/translate_a/single?client=gtx&sl=en&tl=zh-TW&dt=t&q={encoded_text}"
                with urllib.request.urlopen(url, timeout=10) as response:
                    result = json.loads(response.read().decode('utf-8'))
                if result and len(result) > 0 and len(result[0]) > 0:
                    translated_text = result[0][0][0]
                    return translated_text
                else:
                    return "ï¼ˆç¿»è­¯å¤±æ•—ï¼‰"
            except Exception as e:
                return "ï¼ˆç¿»è­¯å¤±æ•—ï¼‰"
        
        def format_chinese_summary(text):
            if not text:
                return text
            import re
            formatted = re.sub(r'([ã€‚ï¼ï¼Ÿ])\s*', r'\1\n\n', text)
            return formatted
        
        # ç²å–æ–°è
        today = date.today()
        yesterday = today - timedelta(days=1)
        url = f"https://newsapi.org/v2/everything?q=finance+OR+earnings+OR+revenue+AI&from={yesterday}&to={today}&sortBy=publishedAt&apiKey={news_api_key}&pageSize=100&language=en"
        
        try:
            response = requests.get(url, timeout=10)
            data = response.json()
            articles = data.get("articles", [])[:50]
        except Exception as e:
            print(f"Error fetching news: {e}")
            articles = []

        # å¾ RSS ç²å–æ›´å¤šæ–°è
        rss_feeds = [
            "https://finance.yahoo.com/rss/",
            "https://feeds.bloomberg.com/markets/news.rss", 
            "https://www.cnbc.com/id/100003114/device/rss/rss.html",
            "https://feeds.reuters.com/reuters/topNews",
            "https://techcrunch.com/feed/"
        ]
        
        for rss_url in rss_feeds:
            try:
                feed = feedparser.parse(rss_url)
                print(f"RSS {rss_url}: {len(feed.entries)} entries found")
                source_name = rss_url.split('//')[1].split('/')[0]
                if 'yahoo' in source_name:
                    source_name = 'Yahoo Finance'
                elif 'bloomberg' in source_name:
                    source_name = 'Bloomberg'
                elif 'cnbc' in source_name:
                    source_name = 'CNBC'
                elif 'reuters' in source_name:
                    source_name = 'Reuters'
                elif 'techcrunch' in source_name:
                    source_name = 'TechCrunch'
                
                for entry in feed.entries[:10]:  # æ¯å€‹ RSS å– 10 ç¯‡
                    article = {
                        "title": entry.title,
                        "description": getattr(entry, 'summary', ''),
                        "url": entry.link,
                        "source": {"name": source_name},
                        "publishedAt": getattr(entry, 'published', '')
                    }
                    articles.append(article)
            except Exception as e:
                print(f"RSS {rss_url} å¤±æ•—: {e}")
        
        # éæ¿¾æ–°è
        filtered_articles = []
        for a in articles:
            title = a.get('title', '') or ''
            desc = a.get('description', '') or ''
            
            # è·³éæ˜é¡¯çš„å»£å‘Šå…§å®¹
            ad_keywords = ['CNN.com', 'Travel Snapshots', 'iReporter photos', 'Submit your best shots', 'Click here for more photos']
            if desc and any(keyword in desc for keyword in ad_keywords):
                continue
                
            # è·³éæ²’æœ‰æ‘˜è¦çš„æ–‡ç« ï¼ˆé€šå¸¸æ˜¯ä½è³ªé‡çš„feedï¼‰
            if not desc or len(desc.strip()) < 10:
                continue
                
            # éæ¿¾æ¢ä»¶ - æ¨™é¡Œå¿…é ˆåŒ…å«é—œéµå­—
            title_has_keyword = any(keyword.lower() in title.lower() for keyword in KEYWORDS)
            
            # é€šéæ¢ä»¶ï¼šæ¨™é¡Œæœ‰é—œéµå­—
            if title_has_keyword:
                try:
                    if detect(title) in ['en', 'zh']:
                        filtered_articles.append(a)
                except:
                    filtered_articles.append(a)  # å¦‚æœèªè¨€æª¢æ¸¬å¤±æ•—ï¼Œä»ä¿ç•™
        
        filtered_articles = filtered_articles[:30]
        print(f"éæ¿¾å¾Œå‰©é¤˜ {len(filtered_articles)} ç¯‡æ–‡ç« ")
        
        # ç”Ÿæˆæ‘˜è¦
        try:
            generator = pipeline('summarization', model='sshleifer/distilbart-cnn-6-6')
            descriptions = [a.get('description', a['title'])[:512] for a in filtered_articles]
            summaries = generator(descriptions, max_length=120, min_length=50, truncation=True)
            print(f'Generated {len(summaries)} summaries')
        except Exception as e:
            print(f'Summarization failed: {e}')
            summaries = [{'summary_text': a.get('description', a['title'])[:200] + '...'} for a in filtered_articles]
        
        # åˆ†æè¶¨å‹¢
        trends = {}
        for title in [a['title'] for a in filtered_articles]:
            for word in KEYWORDS:
                if word.lower() in title.lower():
                    trends[word] = trends.get(word, 0) + 1
        top_trends = sorted(trends.items(), key=lambda x: x[1], reverse=True)[:3]
        
        # ç¿»è­¯
        chinese_summaries = [translate_to_zh(s['summary_text']) for s in summaries]
        
        # ç”ŸæˆHTML
        trends_str = ', '.join([f'{k}: {v} mentions' for k, v in top_trends]) if top_trends else 'No trends available'
        html = '''<!DOCTYPE html>
<html lang="zh-TW">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Financial Briefing</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, sans-serif;
            line-height: 1.6;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
            background-color: #ffffff;
            color: #333333;
        }
        h1 {
            color: #2c3e50;
            border-bottom: 2px solid #3498db;
            padding-bottom: 10px;
        }
        h2 {
            color: #34495e;
            margin-top: 30px;
            font-size: 1.3em;
        }
        .trends {
            background-color: #ecf0f1;
            padding: 15px;
            border-radius: 5px;
            margin: 20px 0;
            font-weight: bold;
            color: #2c3e50;
        }
        .article {
            margin-bottom: 25px;
            padding: 15px;
            border-left: 4px solid #3498db;
            background-color: #f8f9fa;
        }
        .summary {
            margin: 10px 0;
            color: #555555;
        }
        .chinese-summary {
            background-color: #e8f4f8;
            padding: 10px;
            border-radius: 3px;
            margin-top: 10px;
            color: #2c3e50;
        }
        strong {
            color: #2c3e50;
        }
    </style>
</head>
<body>
'''
        html += f'<h1>ğŸ—ï¸AI Financial Briefing Headlines</h1>\n'
        html += f'<p class="trends">Top Trends: {trends_str}</p>\n'
        
        if filtered_articles:
            for i, (a, s, ch) in enumerate(zip(filtered_articles, summaries, chinese_summaries), 1):
                formatted_ch = format_chinese_summary(ch)
                # Pre-compute the replacement to avoid backslashes in f-string
                safe_formatted_ch = formatted_ch.replace('\n\n', '<br><br>')
                html += f'<div class="article">\n'
                html += f'<h2>{i}. {a["title"]}</h2>\n'
                html += f'<div class="summary"><strong>Summary:</strong> {s["summary_text"]}</div>\n'
                html += f'<div class="chinese-summary"><strong>ä¸­æ–‡æ‘˜è¦:</strong> {safe_formatted_ch}</div>\n'
                html += f'</div>\n'
        else:
            html += '<p>No relevant articles found today.</p>\n'
        
        html += '</body></html>'
        
        with open("Daily AI Financial Briefing.html", "w", encoding="utf-8") as f:
            f.write(html)
        
        # ç”Ÿæˆåœ–è¡¨
        try:
            sources = [a.get('source', {}).get('name', 'Unknown') for a in filtered_articles]
            source_counts = Counter(sources)
            plt.figure(figsize=(8, 6))
            plt.pie(list(source_counts.values()), labels=list(source_counts.keys()), autopct='%1.1f%%')
            plt.title('News Source Distribution')
            plt.savefig('source_distribution.png')
            print('Chart generated successfully')
        except Exception as e:
            print(f'Chart generation failed: {e}')
            # å‰µå»ºä¸€å€‹ç©ºçš„åœ–è¡¨æ–‡ä»¶
            plt.figure(figsize=(8, 6))
            plt.text(0.5, 0.5, 'No data available', ha='center', va='center')
            plt.title('News Source Distribution')
            plt.savefig('source_distribution.png')
        EOF
        
    - name: Generate briefing
      run: python generate_briefing.py
      
    - name: Commit and push changes
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        # æª¢æŸ¥æ˜¯å¦æœ‰æ–°å…§å®¹
        if [ -s "Daily AI Financial Briefing.html" ] && grep -q "Top Trends:" "Daily AI Financial Briefing.html"; then
          # ä¸Ÿæ£„æœ¬åœ°æ›´æ”¹ä»¥ç¢ºä¿ clean working directory
          git reset --hard
          # æ‹‰å–æœ€æ–°æ›´æ”¹
          git pull --rebase origin main
          # é‡æ–°ç”Ÿæˆæ–‡ä»¶ï¼ˆå› ç‚º reset ä¸Ÿæ£„äº†å®ƒå€‘ï¼‰
          python generate_briefing.py
          git add "Daily AI Financial Briefing.html" source_distribution.png
          git commit -m "Update daily briefing $(date +'%Y-%m-%d')" || echo "No changes to commit"
          git push
        else
          echo "No new content to commit - briefing appears to be empty"
        fi
