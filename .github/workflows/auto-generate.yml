name: Daily Briefing Generation

on:
  push:
    paths:
      - '.github/workflows/auto-generate.yml'
  schedule:
    - cron: '0 0 * * *'  # 每天早上8點台灣時間 (00:00 UTC)
  workflow_dispatch:  # 手動觸發

jobs:
  generate:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests feedparser matplotlib transformers torch torchvision langdetect
        
    - name: Create generation script
      run: |
        cat > generate_briefing.py << 'EOF'
        # 簡化的生成腳本 - 核心邏輯隱藏
        import os
        import json
        import requests
        from datetime import date, timedelta
        from collections import Counter
        import matplotlib.pyplot as plt
        from transformers import pipeline
        import urllib.parse
        import urllib.request
        from langdetect import detect
        from difflib import SequenceMatcher
        import subprocess
        
        # 設定
        news_api_key = os.getenv("NEWS_API_KEY")
        
        # 關鍵字列表
        KEYWORDS = ['earnings', 'revenue', 'profit', 'q1', 'q2', 'q3', 'q4', 'beats', 'misses', 'stock', 'market cap', 'valuation', 'shares', 'ai', 'artificial intelligence', 'new product', 'launch', 'partnership', 'investment', 'acquisition', 'google', 'microsoft', 'apple', 'tesla', 'nvidia', 'amazon', 'meta', 'openai', 'trump', 'policy', 'decision', 'neo cloud', 'nebius', 'robotaxi', 'optimus', 'rocket', 'nuclear', 'crypto', 'oklo', 'nukz', 'smr', 'arm', 'msft', 'googl', 'pltr', 'tsla', 'nvda', 'aapl', 'meta', 'google', 'microsoft', 'openai', 'chatgpt', 'tesla', 'nvidia', 'amazon', 'tsmc', 'neo cloud', 'aws', 'blockchain', 'nebius', 'nbis', 'semiconductor', 'csp', 'asic', 'elon musk', 'sam altman', 'morgan stanley', 'trump', 'apple', 'robotaxi', 'energy', 'quantum computing', '5g', 'optimus', 'neuralink', 'spacex', 'fed']
        
        def translate_to_zh(text):
            if not text or len(text.strip()) == 0:
                return "（無摘要）"
            try:
                encoded_text = urllib.parse.quote(text)
                url = f"https://translate.googleapis.com/translate_a/single?client=gtx&sl=en&tl=zh-TW&dt=t&q={encoded_text}"
                with urllib.request.urlopen(url, timeout=10) as response:
                    result = json.loads(response.read().decode('utf-8'))
                if result and len(result) > 0 and len(result[0]) > 0:
                    translated_text = result[0][0][0]
                    return translated_text
                else:
                    return "（翻譯失敗）"
            except Exception as e:
                return "（翻譯失敗）"
        
        def format_chinese_summary(text):
            if not text:
                return text
            import re
            formatted = re.sub(r'([。！？])\s*', r'\1\n\n', text)
            return formatted
        
        # 獲取新聞
        today = date.today()
        yesterday = today - timedelta(days=1)
        url = f"https://newsapi.org/v2/everything?q=finance+OR+earnings+OR+revenue+AI&from={yesterday}&to={today}&sortBy=publishedAt&apiKey={news_api_key}&pageSize=100&language=en"
        
        try:
            response = requests.get(url, timeout=10)
            data = response.json()
            articles = data.get("articles", [])[:50]
        except Exception as e:
            print(f"Error fetching news: {e}")
            articles = []

        # 從 RSS 獲取更多新聞
        rss_feeds = [
            "https://finance.yahoo.com/rss/",
            "https://feeds.bloomberg.com/markets/news.rss", 
            "https://www.cnbc.com/id/100003114/device/rss/rss.html",
            "https://feeds.reuters.com/reuters/topNews",
            "https://techcrunch.com/feed/"
        ]
        
        for rss_url in rss_feeds:
            try:
                feed = feedparser.parse(rss_url)
                source_name = rss_url.split('//')[1].split('/')[0]
                if 'yahoo' in source_name:
                    source_name = 'Yahoo Finance'
                elif 'bloomberg' in source_name:
                    source_name = 'Bloomberg'
                elif 'cnbc' in source_name:
                    source_name = 'CNBC'
                elif 'reuters' in source_name:
                    source_name = 'Reuters'
                elif 'techcrunch' in source_name:
                    source_name = 'TechCrunch'
                
                for entry in feed.entries[:10]:  # 每個 RSS 取 10 篇
                    article = {
                        "title": entry.title,
                        "description": getattr(entry, 'summary', ''),
                        "url": entry.link,
                        "source": {"name": source_name},
                        "publishedAt": getattr(entry, 'published', '')
                    }
                    articles.append(article)
            except Exception as e:
                print(f"RSS {rss_url} 失敗: {e}")
        
        # 過濾新聞
        filtered_articles = []
        for a in articles:
            title = a.get('title', '')
            desc = a.get('description', '')
            
            # 跳過明顯的廣告內容
            ad_keywords = ['CNN.com', 'Travel Snapshots', 'iReporter photos', 'Submit your best shots', 'Click here for more photos']
            if any(keyword in desc for keyword in ad_keywords):
                continue
                
            # 跳過沒有摘要的文章（通常是低質量的feed）
            if not desc or len(desc.strip()) < 10:
                continue
                
            # 過濾條件 - 標題必須包含關鍵字
            title_has_keyword = any(keyword.lower() in title.lower() for keyword in KEYWORDS)
            
            # 通過條件：標題有關鍵字
            if title_has_keyword:
                try:
                    if detect(title) in ['en', 'zh']:
                        filtered_articles.append(a)
                except:
                    filtered_articles.append(a)  # 如果語言檢測失敗，仍保留
        
        filtered_articles = filtered_articles[:30]
        
        # 生成摘要
        generator = pipeline('summarization', model='facebook/bart-large-cnn')
        sentiments = [{'label': 'POSITIVE', 'score': 0.8}] * len(filtered_articles)  # 簡化
        
        descriptions = [a.get('description', a['title'])[:512] for a in filtered_articles]
        summaries = generator(descriptions, max_length=120, min_length=50, truncation=True)
        
        # 分析趨勢
        trends = {}
        for title in [a['title'] for a in filtered_articles]:
            for word in KEYWORDS:
                if word.lower() in title.lower():
                    trends[word] = trends.get(word, 0) + 1
        top_trends = sorted(trends.items(), key=lambda x: x[1], reverse=True)[:3]
        
        # 翻譯
        chinese_summaries = [translate_to_zh(s['summary_text']) for s in summaries]
        
        # 生成HTML
        trends_str = ', '.join([f'{k}: {v} mentions' for k, v in top_trends])
        html = f"""<!DOCTYPE html>
        <html><head><title>AI Financial Briefing</title></head><body>
        <h1>AI Financial Briefing Headlines (Generated on {today})</h1>
        <p>Top Trends: {trends_str}</p>
        """
        
        for i, (a, s, ch) in enumerate(zip(filtered_articles, summaries, chinese_summaries), 1):
            formatted_ch = format_chinese_summary(ch)
            # Pre-compute the replacement to avoid backslashes in f-string
            safe_formatted_ch = formatted_ch.replace('\n\n', '<br><br>')
            html += f"""
            <h2>{i}. {a['title']}</h2>
            <p><strong>Summary:</strong> {s['summary_text']}</p>
            <p><strong>中文摘要:</strong> {safe_formatted_ch}</p>
            """
        
        html += "</body></html>"
        
        with open("Daily AI Financial Briefing.html", "w", encoding="utf-8") as f:
            f.write(html)
        
        # 生成圖表
        sources = [a.get('source', {}).get('name', 'Unknown') for a in filtered_articles]
        source_counts = Counter(sources)
        plt.figure(figsize=(8, 6))
        plt.pie(list(source_counts.values()), labels=list(source_counts.keys()), autopct='%1.1f%%')
        plt.title('News Source Distribution')
        plt.savefig('source_distribution.png')
        EOF
        
    - name: Generate briefing
      run: python generate_briefing.py
      
    - name: Commit and push changes
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        # 檢查是否有新內容
        if [ -s "Daily AI Financial Briefing.html" ] && grep -q "Top Trends:" "Daily AI Financial Briefing.html"; then
          git add "Daily AI Financial Briefing.html" source_distribution.png
          git commit -m "Update daily briefing $(date +'%Y-%m-%d')" || echo "No changes to commit"
          git push
        else
          echo "No new content to commit - briefing appears to be empty"
        fi
